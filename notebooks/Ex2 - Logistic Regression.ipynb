{
 "metadata": {
  "name": "",
  "signature": "sha256:b98c86b38c31e035706f2e5d99d2db353690abed7df0e94e56132698e852abb6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Exercise 2\n",
      "In this exercise, you will implement logistic regression and apply it to two different datasets. Before starting on the programming exercise, we strongly recommend watching the video lectures and completing the review questions for the associated topics."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import numpy for linear algebra and numerical computing functions, and matplotlib for plotting graphs\n",
      "import numpy as np\n",
      "from numpy import ones, zeros, newaxis, r_, c_, mat, dot, e, size, log\n",
      "from numpy.linalg import pinv\n",
      "from scipy import optimize\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from IPython.html import widgets\n",
      "from IPython.html.widgets import *\n",
      "from IPython.display import display\n",
      "\n",
      "# Enable matplotlib inline plotting for this notebook\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Logistic Regression\n",
      "In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.\n",
      "Suppose that you are the administrator of a university department and you want to determine each applicant\u2019s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant\u2019s scores on two exams and the admissions decision.\n",
      "Your task is to build a classification model that estimates an applicant\u2019s probability of admission based the scores from those two exams. This outline will guide you through the exercise."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = np.loadtxt('../data/ex2data1.txt', delimiter=',')\n",
      "X = mat(data[:, :2]) # training example inputs\n",
      "y = c_[data[:, 2]]   # training example outputs\n",
      "m = X.shape[0]\n",
      "\n",
      "data[:10] # First 10 rows of training examples (just for viewing)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Visualizing the data\n",
      "Before starting to implement any learning algorithm, it is always good to visualize the data if possible. The following code will load the data and display it on a 2-dimensional plot by calling the function plotData.\n",
      "You will now complete the code in plotData so that it displays a figure like Figure 1, where the axes are the two exam scores, and the positive and negative examples are shown with different markers.\n",
      "![Figure 1](../data/figures/ex2/fig1.png \"Figure 1: Scatter plot of training data\")"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_data(X, y):\n",
      "    \"\"\"\"Plots the data points x and y into a new figure\n",
      "    plots the data points with + for positive examples\n",
      "    and o for negative examples. X is assumed to be a\n",
      "    Mx2 matric.\n",
      "    \"\"\"\n",
      "\n",
      "    ### YOUR CODE HERE ###\n",
      "    # Instructions: Plot the positive and negative examples on a\n",
      "    #               2D plot, using the option 'k+' for the positive\n",
      "    #               examples and 'ko' for the negative examples.\n",
      "    #\n",
      "    # Hint: To get help with plotting in IPython notebooks, start typing\n",
      "    #       plt.plot( and the press Shift-tab\n",
      "    #       Pressing Shift-tab multiple times will give more verbose help options\n",
      "    \n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# When you are done implementing plot_data(X, y) run this cell\n",
      "# This will take the X array (Population of the city) and plot it against\n",
      "# the output array y (Profit of a food truck in that city)\n",
      "plot_data(X.A, y)\n",
      "plt.ylabel('Exam 1 score')\n",
      "plt.xlabel('Exam 2 score')\n",
      "plt.legend(['Admitted', 'Not admitted'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Implementation\n",
      "\n",
      "### Warmup exercise: sigmoid function\n",
      "Before you start with the actual cost function, recall that the logistic regres- sion hypothesis is defined as:\n",
      "\n",
      "$$h_\\theta(x) = g\\left(\\theta^Tx\\right)$$\n",
      "\n",
      "where function $g$ is the sigmoid function. The sigmoid function is defined as:\n",
      "\n",
      "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
      "\n",
      "\n",
      "Your first step is to implement this function so it can be called by the rest of your program. When you are finished, try testing a few values by calling sigmoid(x) in the following cell. For large positive values of x, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. Evaluating sigmoid(0) should give you exactly 0.5. Your code should also work with vectors and matrices. **For a matrix, your function should perform the sigmoid function on every element.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(z):\n",
      "    \"\"\"Compute sigmoid function\n",
      "    computes the sigmoid of z.\n",
      "    \"\"\"\n",
      "\n",
      "   # You need to return the following variables correctly \n",
      "    g = zeros(size(z))\n",
      "\n",
      "    ### YOUR CODE HERE ###\n",
      "    # Instructions: Compute the sigmoid of each value of z (z can be a matrix,\n",
      "    #               vector or scalar).\n",
      "    \n",
      "    return g"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# try testing a few values by calling sigmoid(x)\n",
      "print(\"Sigmoid of 0 should be 0.5, Actual = {}\".format(sigmoid(0)))\n",
      "print(\"Sigmoid of large postive values should be close to 1, Actual = {}\".format(sigmoid(1000)))\n",
      "print(\"Sigmoid of large negative values should be close to 0, Actual = {}\".format(sigmoid(-1000)))\n",
      "\n",
      "# z can be a matrix, vector or scalar\n",
      "print(\"matrix: {}\".format(sigmoid(np.mat([0, 1000, -1000]))))\n",
      "print(\"vector: {}\".format(sigmoid(np.array([0, 1000, -1000]))))\n",
      "print(\"scalar: {}\".format(sigmoid(0)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Cost function and gradient\n",
      "Now you will implement the cost function and gradient for logistic regression.\n",
      "Complete the code in the following cell to return the cost and gradient.\n",
      "\n",
      "Recall that the cost function in logistic regression is\n",
      "\n",
      "$$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\lbrack-y^{(i)}log(h_\\theta(x^{(i)})) - (1-y^{(i)})log(1-h_\\theta(x^{(i)}))\\rbrack$$\n",
      "\n",
      "and the gradient of the cose is a vector of the same length as $\\theta$ where the $j^{th}$ element (for $j = 0,1,...,n$) is defined as follows:\n",
      "\n",
      "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^{m}\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_j^{(i)}$$\n",
      "\n",
      "Note that while this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $h_\\theta(x)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cost_function(theta, X, y):\n",
      "    \"\"\"Compute cost and gradient for logistic regression\n",
      "    computes the cost of using theta as the parameter for\n",
      "    logistic regression and the graident of the cost\n",
      "    with regards to the parameters.\n",
      "    \"\"\"\n",
      "    \n",
      "    # Initialize some useful values\n",
      "    m = y.shape[0] # number of training examples\n",
      "    \n",
      "    # You need to return the following variables correctly\n",
      "    J = 0\n",
      "    grad = zeros(np.size(theta))\n",
      "    \n",
      "    ### YOUR CODE HERE ###\n",
      "    # Instructions: Compute the cost of a particular choice of theta.\n",
      "    #               You should set J to the cost.\n",
      "    #               Compute the partial derivatives and set grad to the partial\n",
      "    #               derivatives of the cost w.r.t. each parameter in theta\n",
      "    #\n",
      "    # Note: grad should have the same dimensions as theta\n",
      "    #\n",
      "    \n",
      "    # I returned J.item(0) and grad.A[0] to get the elements, not the arrays\n",
      "    return J, grad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once you are done the following cell will call your cost_function using the initial parameters of $\\theta$. You should see that the cost is about 0.693."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = mat(data[:, :2]) # Original X\n",
      "X = c_[ones(m), X] # Add a row of ones\n",
      "theta = zeros(X.shape[1])\n",
      "\n",
      "cost, grad = cost_function(theta, X, y)\n",
      "print(cost)\n",
      "print(grad)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Learning parameters using fminunc\n",
      "\n",
      "In the previous assignment, you found the optimal parameters of a linear regression model by implementing gradent descent. You wrote a cost function and calculated its gradient, then took a gradient descent step accordingly. This time, instead of taking gradient descent steps, you will use a Numpy function called `fmin`.\n",
      "\n",
      "Scipy\u2019s `optimize` module contains built-in optimization solvers that finds the minimum of an unconstrained function. For logistic regression, you want to optimize the cost function $J(\\theta)$ with parameters $\\theta$.\n",
      "\n",
      "Concretely, you are going to use `fmin` to find the best parameters $\\theta$ for the logistic regression cost function, given a fixed dataset (of X and y values). You will pass to `fmin` the following inputs:\n",
      "\n",
      "* The initial values of the parameters we are trying to optimize.\n",
      "* A function that, when given the training set and a particular $\\theta$, computes the logistic regression cost and gradient with respect to $\\theta$ for the dataset (X, y)\n",
      "\n",
      "The call to `fmin` will look like this\n",
      "\n",
      "```python\n",
      "#  Set options for fmin\n",
      "options = {'full_output': True, 'maxiter', 400}\n",
      "\n",
      "#  Run fminunc to obtain the optimal theta\n",
      "#  This function will return theta, the final cost J, and the number of iterations performed\n",
      "cost, theta, _, _, _ = optimize.fmin(lambda t: cost_function(t, X, y)[0], initial_theta, **options)\n",
      "```\n",
      "    \n",
      "Here we first defined the options to be used with `fmin`. Additional output is needed to return the gradient out of `fmin`. In `fmin` we do this by setting `full_output` to `True`. This allows `fmin` to use the gradient when minimizing the function. Furthermore, we set the `maxiter` option to 400, so that `fmin` will run for at most 400 steps before it terminates.\n",
      "\n",
      "To specify the actual function we are minimizing, we use a special syntax for specifying that an argument is a function with\n",
      "\n",
      "```python\n",
      "lambda t: cost_function(t, X, y)[0], initial_theta\n",
      "```\n",
      "\n",
      "This creates a function, with argument t, which calls your `cost_function`. This allows us to wrap the cost function for use with fmin. The `[0]` is needed because our `cost_function` returns a tuple of `(cost, gradient)` and `fmin` is expecting only the cost.\n",
      "\n",
      "The underscores in `cost, theta, _, _, _ = ...` are for specifying that those parts of the returned result can be ignored. The full output also returns the number of iterations the algorithm took, the number of function calls made, and warning flags. Therefore an equivalent way of writing this would be `cost, theta, iters, funcalls, warnflag = optimize.fmin(...`\n",
      "\n",
      "If you have completed the cost_function correctly, `fmin` will converge on the right optimization parameters and return the final values of the cost and $\\theta$. Notice that by using `fmin`, you did not have to write any loops yourself, or set a learning rate like you did for gradient descent. This is all done by `fmin`: you only needed to provide a function calculating the cost and the gradient.\n",
      "\n",
      "**Note:**\n",
      "Constraints in optimization often refer to constraints on the parameters, for example, constraints that bound the possible values \u03b8 can take (e.g., \u03b8 \u2264 1). Logistic regression does not have such constraints since \u03b8 is allowed to take any real value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = mat(data[:, :2]) # Original X\n",
      "X = c_[ones(m), X] # Add a row of ones\n",
      "initial_theta = zeros(X.shape[1])\n",
      "options = {'full_output': True, 'maxiter': 400}\n",
      "\n",
      "theta, cost, _, _, _ = optimize.fmin(lambda t: cost_function(t, X, y)[0], initial_theta, **options)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once `fmin` completes, the next cell will call your cost_function function using the optimal parameters of $\\theta$. You should see that the cost is about 0.203.\n",
      "This final $\\theta$ value will then be used to plot the decision boundary on the training data, resulting in a figure similar to Figure 2. We also encourage you to look at the code in this cell to see how to plot such a boundary using the $\\theta$ values.\n",
      "\n",
      "![Figure 2](../data/figures/ex2/fig2.png \"Figure 2: Training data with decision boundary\")"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_decision_boundary(theta, X, y):\n",
      "    plot_data(X[:, 1:3], y)\n",
      " \n",
      "    if X.shape[1] <= 3:\n",
      "        plot_x = r_[X[:,2].min()-2,  X[:,2].max()+2]\n",
      "        plot_y = (-1./theta[2]) * (theta[1]*plot_x + theta[0])\n",
      " \n",
      "        plt.plot(plot_x, plot_y)\n",
      "        plt.legend(['Admitted', 'Not admitted', 'Decision Boundary'])\n",
      "        plt.axis([30, 100, 30, 100])\n",
      "    else:\n",
      "        pass\n",
      "\n",
      "plot_decision_boundary(theta, X.A, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Evaluating logistic regression\n",
      "After learning the parameters, you can use the model to predict whether a particular student will be admitted. For a student with an Exam 1 score of 45 and an Exam 2 score of 85, you should expect to see an admission probability of 0.776.\n",
      "\n",
      "Another way to evaluate the quality of the parameters we have found is to see how well the learned model predicts on our training set. In this part, your task is to complete the code in the next cell. The predict function will produce \u201c1\u201d or \u201c0\u201d predictions given a dataset and a learned parameter vector $\\theta$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(theta, X):\n",
      "    \"\"\"Predict whether the label is 0 or 1 using learned logistic \n",
      "    regression parameters theta\n",
      "    computes the predictions for X using a threshold at 0.5\n",
      "    (i.e., if sigmoid(theta'*x) >= 0.5, predict 1)\n",
      "    \"\"\"\n",
      "\n",
      "    # You need to return the following variables correctly\n",
      "    p = zeros(X.shape[0])\n",
      "\n",
      "    ### YOUR CODE HERE ###\n",
      "    # Instructions: Complete the following code to make predictions using\n",
      "    #               your learned logistic regression parameters. \n",
      "    #               You should set p to a vector of 0's and 1's\n",
      "    \n",
      "    return p"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After you have completed the code, this cell will proceed to report the training accuracy of your classifier by computing the percentage of examples it got correct."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prob = sigmoid(mat('1 45 85') * c_[theta])\n",
      "print(\"For a student with scores 45 and 85, we predict an admission probability of {:.1%}\".format(prob.item(0)))\n",
      "\n",
      "predictions = predict(theta, X)\n",
      "accuracy = (predictions == y).mean()\n",
      "print('Training Accuracy: {:.1%}'.format(accuracy))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}